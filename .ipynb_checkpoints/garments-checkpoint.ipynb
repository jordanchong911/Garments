{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garments Project (by: The Guo Family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Garmets are worn every day by humans around the world. From the famous and highly sought after fashion houses such as the Haus of Gucci, Louis Vitton, Dior, down to the smaller fashion labels, humans have the tendency to buy what is on the shelf. But, behind all the garments that are sold on a day to day basis, what goes behind the scenes? In this project, we want to look at the reasoning between the overtime that happens in this current fashion house together with making three (3) models to predict the number of overtime that will happen in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('garments.csv') # Importing garments.csv file to a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the dataset into the dataframe, we now look for errors and see what can we do for null cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure\n",
    "print(df.info())  # Column names, data types, and non-null counts\n",
    "print(df.describe())  # Statistics for numeric columns\n",
    "print(df.head())  # Preview the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are null values in the WIP column, highlighted in the figure below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize missing data\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Data Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The team chose to replace empty cells with zero. This is doable since WIP stands for \"work in progress\" if the cell is null, it means that there are no projects that are \"work in progress\", making them null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Replace empty strings with 0\n",
    "df = df.replace('', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize missing data\n",
    "msno.matrix(df)\n",
    "plt.title('Missing Data Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're making sure that the dataset does not have any more null cells.\n",
    "\n",
    "Afterwards, we normalize the values to make sure that all numerical features have the same impact on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED BY MATT\n",
    "\n",
    "# true_numeric_columns = ['over_time', 'smv', 'wip', 'incentive', 'idle_time', 'idle_men', \n",
    "#                         'no_of_style_change', 'no_of_workers']\n",
    "\n",
    "# numeric_columns = df[true_numeric_columns]\n",
    "\n",
    "# # Initialize the MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Normalize the numeric columns and update the DataFrame\n",
    "# df[numeric_columns.columns] = scaler.fit_transform(numeric_columns)\n",
    "\n",
    "#modified by matt to get scaler for denormalization\n",
    "true_numeric_columns = ['over_time', 'smv', 'wip', 'incentive', 'idle_time', 'idle_men', \n",
    "                        'no_of_style_change', 'no_of_workers']\n",
    "\n",
    "everything_else_columns = ['smv', 'wip', 'incentive', 'idle_time', 'idle_men', \n",
    "                        'no_of_style_change', 'no_of_workers']\n",
    "\n",
    "numeric_columns = df[true_numeric_columns]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# added by matt\n",
    "over_time_scaler = MinMaxScaler() # Separate scaler so we can use it later for denormalization\n",
    "df['over_time'] = over_time_scaler.fit_transform(df[['over_time']])\n",
    "\n",
    "# Normalize the non-over_time numeric columns and update the DataFrame\n",
    "df[everything_else_columns] = scaler.fit_transform(df[everything_else_columns])\n",
    "\n",
    "joblib.dump(over_time_scaler, 'over_time_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing the values, we move on to adding some columns to help with parsing through data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['month'] = df['date'].dt.month  # Extract the month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we move on to looking at the data itself and see what does it mean to this certain fashion haus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we look at the distribution of each numerical data and see what it means to the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_numeric_columns = ['over_time', 'smv', 'wip', 'incentive', 'idle_time', 'idle_men', \n",
    "                        'no_of_style_change', 'no_of_workers', 'actual_productivity', 'targeted_productivity']\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"figures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Starting figure number\n",
    "figure_number = 2.1\n",
    "\n",
    "# Starting figure number\n",
    "major_figure_number = 2  # Represents the major part (e.g., 2.x)\n",
    "minor_figure_number = 1  # Represents the minor part (e.g., x.1)\n",
    "\n",
    "# Visualize distributions of numeric columns\n",
    "for col in true_numeric_columns:\n",
    "    figure_number = f\"{major_figure_number}.{minor_figure_number}\" # Combine major and minor figure numbers for the filename\n",
    "    plt.figure(figsize=(14, 4))  # Adjust figure size\n",
    "    sns.boxplot(x='month', y=col, data=df, showfliers=True)  # Boxplot\n",
    "    sns.stripplot(x='month', y=col, data=df, color='black', alpha=0.5, jitter=0.1, dodge=False)  # Reduced jitter\n",
    "    plt.title(f'Distribution of {col} by Month', fontsize=16, fontweight='bold')  # Title\n",
    "    plt.xlabel('Month', fontsize=14)\n",
    "    plt.ylabel(col, fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add a grid for better readability\n",
    "    plt.figtext(0.5, -0.1, f\"Figure {figure_number}: Distribution of {col} by Month\", ha='center', fontsize=12) # Add figure label as a caption\n",
    "    \n",
    "    \n",
    "    figure_filename = os.path.join(output_dir, f\"Figure_{figure_number}.png\")\n",
    "    plt.savefig(figure_filename, bbox_inches='tight')\n",
    "    \n",
    "    # Increment the minor figure number\n",
    "    minor_figure_number += 1\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in Figure 2.1, it shows the distribution of over time per month. The chart shows different patterns in overtime hours over three months. In Month 1, the median overtime is higher and more varied than in Months 2 and 3. This might mean there's a seasonal pattern, like a busy time with more work or production, possibly due to certain needs or deadlines. Also, some extreme values in Month 1, almost reaching the highest possible overtime of 1.0, could be due to unusual events, like sudden staff shortages or urgent project needs, causing some workers or days to have much higher overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns excluding the true numeric ones\n",
    "remaining_columns = [col for col in df.columns if col not in true_numeric_columns and col != 'date']\n",
    "\n",
    "# Filter the remaining categorical columns (assuming they're not numeric)\n",
    "categorical_columns = df[remaining_columns]\n",
    "\n",
    "# Check unique values and distributions for the remaining categorical features\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(14, 4))  # Width: 14 inches, Height: 8 inches\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Correlations with 'over_time'\n",
    "correlation_with_overtime = numeric_columns.corr()['over_time'].sort_values(ascending=False)\n",
    "print(\"Correlation with Overtime:\\n\", correlation_with_overtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots for numeric columns\n",
    "for col in ['no_of_workers', 'smv', 'wip', 'incentive']:\n",
    "    sns.scatterplot(x=col, y='over_time', data=df)\n",
    "    plt.title(f'Overtime vs {col}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for categorical features\n",
    "for col in ['quarter', 'department', 'day']:\n",
    "    sns.boxplot(x=col, y='over_time', data=df)\n",
    "    plt.title(f'Overtime by {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction terms (e.g., workers_per_smv)\n",
    "df['workers_per_smv'] = df['no_of_workers'] / (df['smv'] + 1e-6)\n",
    "sns.scatterplot(x='workers_per_smv', y='over_time', data=df)\n",
    "plt.title('Overtime vs Workers per SMV')\n",
    "plt.show()\n",
    "\n",
    "# Group-Wise Analysis\n",
    "grouped = df.groupby('quarter').agg({\n",
    "    'over_time': ['mean', 'std'],\n",
    "    'targeted_productivity': ['mean'],\n",
    "    'actual_productivity': ['mean']\n",
    "})\n",
    "print(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the 'workers_per_smv' column\n",
    "df['workers_per_smv'] = scaler.fit_transform(df[['workers_per_smv']])\n",
    "\n",
    "# Define the columns to include in the boxplot\n",
    "selected_columns = ['over_time', 'smv', 'wip', 'incentive', 'idle_time', 'idle_men', \n",
    "                    'no_of_style_change', 'no_of_workers', 'workers_per_smv']\n",
    "\n",
    "# Boxplot for selected numeric features\n",
    "plt.figure(figsize=(14, 8))  # Width: 14 inches, Height: 8 inches\n",
    "sns.boxplot(data=df[selected_columns])  # Use only the selected columns\n",
    "plt.title('Box Plot for Selected Numeric Features')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()\n",
    "\n",
    "# Z-score method for outlier detection\n",
    "df['z_score'] = zscore(df['over_time'])\n",
    "outliers = df[df['z_score'].abs() > 3]\n",
    "print(\"Outliers:\\n\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends over time\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "df['over_time'].resample('M').mean().plot()\n",
    "plt.title('Monthly Overtime Trends')\n",
    "plt.ylabel('Overtime (Mean)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from the 'department' column\n",
    "df['department'] = df['department'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for those who will merge:** My model 2 code requires \"import joblib\" at the start of the nb and the cell marked \"MODIFIED BY MATT\" to remain intact :))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MODEL 2] Predicting overtime using a neural network (Matthew Ong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be predicting *over_time* using a PyTorch neural network, which we will build as we go along. We will be using all of the other columns from the dataset as the model's features, with the exception of date (which will be explained later).\n",
    "\n",
    "The reason we chose a neural network as one of our machine learning models for this problem is because, as seen in the EDA, **many features in the dataset individually have little-to-no correlation with *over_time***. While this might typically pose a challenge, **neural networks are able to capture the subtle, non-linear relationships** that might exist where a combination of those features might have a much bigger impact than any one of them alone. Its ability to learn such patterns is the main reason we are using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first import all the needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from garments import DataLoader\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our dataframe with the preprocessed data, we'll assign the features to *X* and the target (over_time) to *y*. \n",
    "\n",
    "We will not be including the date feature as we feel that it isn't meaningful to the model as is. Additionally, the year component is irrelevant since all records are from the same year anyway (2015). Instead, the network will be using the derived **month column** alongside the quarter and day columns to capture any potential temporal trends.\n",
    "\n",
    "We also remove the columns added during EDA, namely z_score and workers_per_smv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn = df.reset_index() # remove the date as index\n",
    "X = df_nn.drop(columns=['over_time', 'z_score', 'workers_per_smv', 'date'])\n",
    "y = df_nn['over_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(X.shape)\n",
    "print(X)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to one-hot encode the categorical features (quarter, department, and day) because neural networks can only work with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X, columns=['quarter', 'department', 'day'])\n",
    "X = X.astype(float) # convert the True/False into 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(X.shape)\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train-test-validation split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now divide our data into train, test, and validation sets following a 64-20-16 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split (X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(\"Train set:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[:10])\n",
    "print(\"\\nTest set:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y_test[:10])\n",
    "print(\"\\nValidation set:\")\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(y_val[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test, and validation data are converted from pandas DataFrames into PyTorch tensors for use in the neural network later. Conversion to numpy arrays is an intermediate step as there is no direct conversion between DFs and tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up the neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been processed and split into train, test, and validation, we can now set up the neural network. Before that, we will use DataLoader to batch our training data as we will be performing mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(X_train, y_train, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a class with the functions needed to build the neural network. This was taken from Lab 5 but with modifications made to suit regression and our particular task. These modifications include changing the hidden layer activation function to ReLU, removing the softmax layer, changing initialization to He, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, list_hidden):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.list_hidden = list_hidden\n",
    "\n",
    "    def create_network(self):\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(in_features=self.input_size, out_features=self.list_hidden[0]))\n",
    "        layers.append(nn.ReLU()) # made it always use ReLu as the activation function for hidden layers\n",
    "\n",
    "        for i in range(len(self.list_hidden) - 1):\n",
    "            layers.append(torch.nn.Linear(in_features=self.list_hidden[i], out_features=self.list_hidden[i+1]))\n",
    "            layers.append(nn.ReLU()) # made it always use ReLu as the activation function for hidden layers\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_features=self.list_hidden[-1], out_features=1)) # made output neuron always one\n",
    "        self.layers = nn.Sequential(*layers) # removed softmax layer\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.manual_seed(2)\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu') # changed initialization to He since we're using ReLU\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Output of layer {i}:', x, '\\n')\n",
    "\n",
    "        return x  # final output for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare the different hyperparameter variations that we will be considering for tuning. In this problem, we will be tuning the neural network structure (the number of hidden layers and nodes in each hidden layer) and the ADAM optimizer learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structures = [[8], [16,], [32], [16, 8], [32, 16], [32, 16, 8]]\n",
    "adam_learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set our loss function to Mean Squared Error (MSE) as it is a common choice for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use everything we've just defined to build a *train_and_validate* function that allows us to train the model on the training data, then test it on the validation data. The different configurations of network structure + optimizer learning rate will be **compared based on their validation set loss**, and the configuration with the lowest loss will be selected as the optimum setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate (train_dataloader, X, y, learning_rate, network_structure, test=False, verbose=False):\n",
    "    max_epochs = 300\n",
    "    min_change = 0.000003\n",
    "    this_epoch = 0\n",
    "    is_converged = False\n",
    "    prev_loss = 0.0\n",
    "    \n",
    "    ann = NeuralNetwork(input_size=24, num_classes=1, list_hidden=network_structure) # we will tune the network structure\n",
    "    ann.create_network() # builds the neural network\n",
    "    ann.init_weights() # initializes the weights according to He initialization\n",
    "    \n",
    "    optimizer = optim.Adam(ann.parameters(), lr=learning_rate) # we will tune the learning rate\n",
    "\n",
    "    # train the model\n",
    "    ann.train() # sets the model to train mode\n",
    "    while this_epoch < max_epochs and is_converged is not True:\n",
    "        this_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in zip(*train_dataloader.get_batch(mode='train')):\n",
    "            optimizer.zero_grad() # empties the gradients\n",
    "            scores = ann.forward(X_batch) # forward propagation\n",
    "            loss = criterion(scores, y_batch.view(-1, 1)) # get the loss\n",
    "            loss.backward() # backpropagation\n",
    "            optimizer.step() # update the weights\n",
    "            \n",
    "            this_loss += loss.item()\n",
    "\n",
    "        average_loss = this_loss / len(X_batch)\n",
    "\n",
    "        if abs(prev_loss - average_loss) < min_change: # stop training early if the change in loss is less than a given minimum\n",
    "            is_converged = True\n",
    "            print(f\"Training stopped early due to convergence (Epoch: {this_epoch + 1}).\")\n",
    "            print(f\"Loss difference vs previous epoch: {abs(prev_loss - average_loss):.10f}\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            prev_loss = average_loss\n",
    "            this_epoch+=1\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {this_epoch}/{max_epochs}, Loss: {average_loss}')\n",
    "\n",
    "\n",
    "    # evaluate the model (using the validation set)\n",
    "    ann.eval()\n",
    "\n",
    "    with torch.no_grad(): # PyTorch doesn't need to track gradients since we're just evaluating (no updating weights)\n",
    "        final_scores = ann.forward(X)\n",
    "        final_loss = criterion(final_scores, y.view(-1, 1))\n",
    "        \n",
    "    if test is False:\n",
    "        print(f\"Validation Loss: {final_loss}\")\n",
    "        return final_loss  \n",
    "\n",
    "    else:\n",
    "        print(f\"Test Loss: {final_loss}\")\n",
    "\n",
    "    return final_loss, final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform our hyperparameter tuning proper where we will call our *train_and_validate* function multiple times, each time with a different combination of network structure and optimizer learning rate. The one which produces the best validation set loss will be used in our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_tuning ():\n",
    "    best_val_loss = 100000\n",
    "    best_network_structure = None\n",
    "    best_adam_learning_rate = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for learning_rate in adam_learning_rates:\n",
    "        for structure in network_structures:\n",
    "            print(f\"Learning rate: {learning_rate} & Network structure: {structure}\")\n",
    "            val_loss = train_and_validate(train_dataloader, X_val, y_val, learning_rate, structure, verbose=False) # getting each configuration's val loss\n",
    "    \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_adam_learning_rate = learning_rate\n",
    "                best_network_structure = structure\n",
    "                \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Best Learning Rate: {best_adam_learning_rate}\")\n",
    "    print(f\"Best Hidden Layer Configuration: {best_network_structure}\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss}\")\n",
    "    print(f\"Total hyperparameter tuning time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return best_adam_learning_rate, best_network_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_adam_learning_rate, best_network_structure = hyperparam_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having performed the hyperparameter tuning, we now know the best network structure and ADAM optimizer learning rate for our model (at least on the validation set). At this point, we can finally train and test our optimized model on the test set using those parameters. Once this is done, we print the loss and the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "test_loss, test_scores = train_and_validate(train_dataloader, X_test, y_test, best_adam_learning_rate, best_network_structure, test=True, verbose=False)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Final model training and testing time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 10\n",
    "print(test_loss)\n",
    "print(test_scores.shape)\n",
    "print(test_scores[:numberOfRows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the final result? The first thing we have to do is to denormalize the model's predictions using the same scaler that was used to normalize the targets earlier in the notebook. We also denormalize y_test so we can compare the predictions to it in the original scale (minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overtimescaler = joblib.load('over_time_scaler.pkl')\n",
    "test_scores_denormalized = overtimescaler.inverse_transform(test_scores.reshape(-1, 1))\n",
    "y_test_denormalized = overtimescaler.inverse_transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 15\n",
    "for i in range(numberOfRows):\n",
    "    print(f\"Prediction: {test_scores_denormalized[i]}, True Value: {y_test_denormalized[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also take the square root of the MSE to get the RMSE, then bring it into the original scale (minutes) just like the predictions by denormalizing it. **This value is the amount of minutes the model's predictions are off by, on average.** For example, an RMSE of 500 means that the predictions are usually off by around 500 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = torch.sqrt(test_loss)\n",
    "rmse_denormalized = overtimescaler.inverse_transform(rmse.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(f\"MSE: {test_loss}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"RMSE denormalized (minutes): {rmse_denormalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize how the predictions compare to the correct answers by using a scatterplot. Each blue dot represents a prediction, and their distance from the red line is the error or RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_denormalized, test_scores_denormalized, alpha=0.5, color='blue')\n",
    "plt.plot([min(y_test_denormalized), max(y_test_denormalized)], [min(y_test_denormalized), max(y_test_denormalized)], color='red', label='Perfect prediction')\n",
    "plt.xlabel('Actual Overtime (minutes)')\n",
    "plt.ylabel('Predicted Overtime (minutes)')\n",
    "plt.title(f'Original model (RMSE: {rmse_denormalized[0,0]} minutes)') # used the index so it wouldn't be inside square brackets\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improving the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the hyperparameter tuning we did, the model's predictions are still off by around 1600-2000 minutes on average. This is pretty tragic because the mean of the over_time column is around 4567 minutes, meaning **the predictions are wrong by between a third and half the typical overtime value!** The following section will used to test several theories as to why this is the case and how we might be able to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPROVEMENT 1 : Binning & Stratification** \n",
    "\n",
    "*Theory: Perhaps my train, test, and validation splits are not representing the overall dataset well enough.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical targets, we usually stratify when splitting the dataset to ensure the proportions of the classes are similar across the different sets. In this case, it is a little tougher because we are dealing with continuous values, but we can get around that by binning *over_time* so that they become categorical. \n",
    "\n",
    "There are two types of binning: equal-width (better when the target instances are evenly distributed) and equal-frequency (better when the target instances are skewed). Thus, before we bin, we must first measure the skewness of *over_time*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_time_skewness = skew(df_nn['over_time'])\n",
    "print(over_time_skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a skewness of around 0.672, *over_time* is moderately positively skewed, meaning that more data is concentrated around the lower end. As such, we will choose equal-frequency binning. We make a temporary column containing only 5 bins, as any higher leads to a very unequal number of instances per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nn['over_time_bins'] = pd.qcut(df_nn['over_time'], q=5, labels=False)\n",
    "print(df_nn['over_time_bins'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done the binning, we can add the column to X amd perform the train_test split again, stratifying by *over_time_bins*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape before adding over_time_bins: {X.shape}')\n",
    "X['over_time_bins'] = df_nn['over_time_bins']\n",
    "print(f'Shape after adding over_time_bins: {X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=0, stratify=X['over_time_bins'])\n",
    "X_train, X_val, y_train, y_val = train_test_split (X_train, y_train, test_size=0.2, random_state=0, stratify=X_train['over_time_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(\"Train bins distribution:\", np.unique(X_train['over_time_bins'], return_counts=True))\n",
    "print(\"Val bins distribution:\", np.unique(X_val['over_time_bins'], return_counts=True))\n",
    "print(\"Test bins distribution:\", np.unique(X_test['over_time_bins'], return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must not forget to drop *over_time_bins* from our sets after splitting as it isn't a feature for our neural network, it's just a temporary column we used to stratify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['over_time_bins'])\n",
    "X_val = X_val.drop(columns=['over_time_bins'])\n",
    "X_test = X_test.drop(columns=['over_time_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed as usual with our process for preparing and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# use dataloader for the train set\n",
    "train_dataloader = DataLoader(X_train, y_train, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "best_adam_learning_rate, best_network_structure = hyperparam_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "bs_test_loss, bs_test_scores = train_and_validate(train_dataloader, X_test, y_test, best_adam_learning_rate, best_network_structure, test=True, verbose=False)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Final model training and testing time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 10\n",
    "print(bs_test_loss)\n",
    "print(bs_test_scores.shape)\n",
    "print(bs_test_scores[:numberOfRows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that our model with binning and stratification is converging much faster than the original, as the training times are lower due to many more of the hyperparameter tuning runs hitting the minimum loss reduction before reaching the final epoch. Let's take a look at how it performed by getting its predictions and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_test_scores_denormalized = overtimescaler.inverse_transform(bs_test_scores.reshape(-1, 1))\n",
    "bs_y_test_denormalized = overtimescaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "bs_rmse = torch.sqrt(bs_test_loss)\n",
    "bs_rmse_denormalized = overtimescaler.inverse_transform(bs_rmse.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 15\n",
    "for i in range(numberOfRows):\n",
    "    print(f\"Prediction: {bs_test_scores_denormalized[i]}, True Value: {bs_y_test_denormalized[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(f\"MSE: {bs_test_loss}\")\n",
    "print(f\"RMSE: {bs_rmse}\")\n",
    "print(f\"RMSE denormalized (minutes): {bs_rmse_denormalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model tends to perform slightly better than the original model! Let's visualize this using a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bs_y_test_denormalized, bs_test_scores_denormalized, alpha=0.5, color='blue')\n",
    "plt.plot([min(bs_y_test_denormalized), max(bs_y_test_denormalized)], [min(bs_y_test_denormalized), max(bs_y_test_denormalized)], color='red', label='Perfect prediction')\n",
    "plt.xlabel('Actual Overtime (minutes)')\n",
    "plt.ylabel('Predicted Overtime (minutes)')\n",
    "plt.title(f'Model w/ B&S (RMSE: {bs_rmse_denormalized[0,0]} minutes)') # used the index so it wouldn't be inside square brackets\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPROVEMENT 2 : K-fold Cross-Validation** \n",
    "\n",
    "*Theory: Perhaps the model is not being fed enough training data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With how we're currently splitting the data (80/20 train/test, then 80/20 train/validation), we are only being left with 64% of the data for the training set. Given that there are less than 1,200 instances in total, the training set might be too small for the model to effectively learn the general trend. Let's try to get around this by removing the validation set entirely and replacing it with K-fold Cross Validation for our hyperparameter tuning and model training.\n",
    "\n",
    "We will be doing this on top of binning and stratification, so let's start by revisiting the our train-test-validation split and making it only split the data into train/test and stratifying by *over_time_bins*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['over_time_bins'] = df_nn['over_time_bins']\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=0, stratify=X['over_time_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(\"Train bins distribution:\", np.unique(X_train['over_time_bins'], return_counts=True))\n",
    "print(\"Test bins distribution:\", np.unique(X_test['over_time_bins'], return_counts=True))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['over_time_bins'])\n",
    "X_test = X_test.drop(columns=['over_time_bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to convert them into tensors first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our X_train and X_test. Next, let's modify our *train_and_validate* function to leverage K-fold Cross Validation for the training and validation of the model. We will be setting the number of folds to 5 to ensure enough data is in each training cycle (5 cycles; in each cycle: train on 4 folds, validate on the last fold).\n",
    "\n",
    "**Note:** This incidentally means that we will be training on 64% of the data again, since training on 4/5 folds of training data that is 80% of the whole dataset is just 80% of 80%, exactly the amount when we had the validation set. The difference here, however, is that we will be training it for 5 cycles with each one being validated on a different fold, meaning that we still maximize the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfcv_train_and_validate (X, y, learning_rate, network_structure, k=5, verbose=False):\n",
    "    max_epochs = 300\n",
    "    min_change = 0.000003\n",
    "    kfcv = KFold(n_splits=k, shuffle=True, random_state=0) # the K-fold object\n",
    "    fold_losses = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfcv.split(X)):\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold + 1} of {k}:\")\n",
    "\n",
    "        X_trainfold, y_trainfold = X[train_idx], y[train_idx] # assigning the four training folds\n",
    "        X_valfold, y_valfold = X[val_idx], y[val_idx] # assigning the validation fold\n",
    "        train_dataloader = DataLoader(X_trainfold, y_trainfold, 16) # makes a new dataloader per fold because the content of train changes each time\n",
    "\n",
    "        ann = NeuralNetwork(input_size=24, num_classes=1, list_hidden=network_structure) # we will tune the network structure\n",
    "        ann.create_network() # builds the neural network\n",
    "        ann.init_weights() # initializes the weights according to He initialization\n",
    "        \n",
    "        optimizer = optim.Adam(ann.parameters(), lr=learning_rate) # we will tune the learning rate\n",
    "\n",
    "        # early stopping is checked and performed per fold\n",
    "        this_epoch = 0\n",
    "        is_converged = False\n",
    "        prev_loss = 0.0\n",
    "    \n",
    "        # train the model\n",
    "        ann.train() # sets the model to train mode\n",
    "        while this_epoch < max_epochs and is_converged is not True:\n",
    "            this_loss = 0.0\n",
    "\n",
    "            for X_batch, y_batch in zip(*train_dataloader.get_batch(mode='train')):\n",
    "                optimizer.zero_grad() # empties the gradients\n",
    "                scores = ann.forward(X_batch) # forward propagation\n",
    "                loss = criterion(scores, y_batch.view(-1, 1)) # get the loss\n",
    "                loss.backward() # backpropagation\n",
    "                optimizer.step() # update the weights\n",
    "                        \n",
    "                this_loss += loss.item()\n",
    "\n",
    "            average_loss = this_loss / len(X_batch)\n",
    "\n",
    "            if abs(prev_loss - average_loss) < min_change: # stop training early if the change in loss is less than a given minimum\n",
    "                is_converged = True\n",
    "                if verbose:\n",
    "                    print(f\"Fold {fold+1} stopped early due to convergence (Epoch: {this_epoch + 1}).\")\n",
    "                    print(f\"Loss difference vs previous epoch: {abs(prev_loss - average_loss):.10f}\")\n",
    "                break\n",
    "    \n",
    "            else:\n",
    "                prev_loss = average_loss\n",
    "                this_epoch+=1\n",
    "\n",
    "            if verbose:\n",
    "                print(f'Fold {fold+1}/{k} | Epoch {this_epoch}/{max_epochs}, Loss: {average_loss}')\n",
    "\n",
    "\n",
    "        # evaluate the model on the validation fold\n",
    "        ann.eval() # sets the model to evaluation mode\n",
    "\n",
    "        with torch.no_grad(): # PyTorch doesn't need to track gradients since we're just evaluating (no updating weights)\n",
    "            fold_scores = ann.forward(X_valfold)\n",
    "            fold_loss = criterion(fold_scores, y_valfold.view(-1, 1))\n",
    "\n",
    "        print(f\"Fold {fold+1} Loss: {fold_loss}\")\n",
    "        fold_losses.append(fold_loss)\n",
    "\n",
    "    allfolds_average_loss = sum(fold_losses)/k\n",
    "    print(f\"Average loss across {k} folds: {allfolds_average_loss}\")\n",
    "\n",
    "    return allfolds_average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also have to slightly update our hyperparameter tuning loop to use *kfcv_train_and_validate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfcv_hyperparam_tuning ():\n",
    "    best_allfolds_average_loss = 100000\n",
    "    best_network_structure = None\n",
    "    best_adam_learning_rate = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for learning_rate in adam_learning_rates:\n",
    "        for structure in network_structures:\n",
    "            print(f\"Learning rate: {learning_rate} & Network structure: {structure}\")\n",
    "            allfolds_average_loss = kfcv_train_and_validate(X_train, y_train, learning_rate, structure, verbose=False) # getting each configuration's val loss\n",
    "    \n",
    "            if allfolds_average_loss < best_allfolds_average_loss:\n",
    "                best_allfolds_average_loss = allfolds_average_loss\n",
    "                best_adam_learning_rate = learning_rate\n",
    "                best_network_structure = structure\n",
    "                \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Best Learning Rate: {best_adam_learning_rate}\")\n",
    "    print(f\"Best Hidden Layer Configuration: {best_network_structure}\")\n",
    "    print(f\"Best Average Loss: {best_allfolds_average_loss}\")\n",
    "    print(f\"Total hyperparameter tuning time: {training_time:.2f} seconds\")\n",
    "\n",
    "    return best_adam_learning_rate, best_network_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform hyperparameter tuning using K-fold Cross Validation and get the best optimizer learning rate and network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_adam_learning_rate, best_network_structure = kfcv_hyperparam_tuning ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the neural network using the selected hyperparameters as usual, but note that we have a lot more test data now since we did not have to use a dedicated validation split for hyperparameter tuning (80% of the dataset is used to train compared to the previous 64%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(X_train, y_train, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "kfcv_test_loss, kfcv_test_scores = train_and_validate(train_dataloader, X_test, y_test, best_adam_learning_rate, best_network_structure, test=True, verbose=False)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Final model training and testing time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 10\n",
    "print(kfcv_test_loss)\n",
    "print(kfcv_test_scores.shape)\n",
    "print(kfcv_test_scores[:numberOfRows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, hyperparameter tuning is taking a significantly longer time to complete when doing K-fold Cross Validation. Since each fold runs up to all 300 epochs (unless it converges early), we can expect our 5-fold tuning to take 5 times as long as the original time taken. True enough, the seconds elapsed went from around 200 to around 1000.\n",
    "\n",
    "Let's take a look at how our model performed by getting its predictions and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfcv_test_scores_denormalized = overtimescaler.inverse_transform(kfcv_test_scores.reshape(-1, 1))\n",
    "kfcv_y_test_denormalized = overtimescaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "kfcv_rmse = torch.sqrt(kfcv_test_loss)\n",
    "kfcv_rmse_denormalized = overtimescaler.inverse_transform(kfcv_rmse.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "numberOfRows = 15\n",
    "for i in range(numberOfRows):\n",
    "    print(f\"Prediction: {kfcv_test_scores_denormalized[i]}, True Value: {kfcv_y_test_denormalized[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "print(f\"MSE: {kfcv_test_loss}\")\n",
    "print(f\"RMSE: {kfcv_rmse}\")\n",
    "print(f\"RMSE denormalized (minutes): {kfcv_rmse_denormalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this improvement has led to the model performing slightly better than the previous one. Let's visualize it on a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kfcv_y_test_denormalized, kfcv_test_scores_denormalized, alpha=0.5, color='blue')\n",
    "plt.plot([min(kfcv_y_test_denormalized), max(kfcv_y_test_denormalized)], [min(kfcv_y_test_denormalized), max(kfcv_y_test_denormalized)], color='red', label='Perfect prediction')\n",
    "plt.xlabel('Actual Overtime (minutes)')\n",
    "plt.ylabel('Predicted Overtime (minutes)')\n",
    "plt.title(f'Model w/ B&S + KFCV (RMSE: {kfcv_rmse_denormalized[0,0]} minutes)') # used the index so it wouldn't be inside square brackets\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sir's advice\n",
    "1. Use all features (features that are not correlated on their own might have some sense when combined with others)\n",
    "2. Consider not normalizing the target AKA over_time\n",
    "3. Since loss is so small, let's look at it as percentage decrease per iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible improvements:\n",
    "\n",
    "1. Bin and stratify when doing train-test-val split **[DONE]**\n",
    "2. Change the split ratios or just use train-test with k-fold validation instead of having a validation set **[DONE]**\n",
    "3. Patience mechanism in the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
